%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************
\chapter{Related Work}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi

\fbox{
    \parbox{\textwidth}
    {
        Chapter Overview
        \begin{itemize}
            \item How Bayesian Methods were applied to Neural Networks for the intractable true posterior distribution.
            \item Various ways of training Neural Networks posterior probability distributions : Laplace approximations, Monte Carlo and Variational Inference.
            \item Proposals on Dropout and Gaussian Dropout as Variational Inference schemes.
            \item Work done in the past for uncertainity estimation in Neural Network.
            \item Ways to reduce the number of parameters in a model.
        \end{itemize}
    }
}

\pagebreak

\section{Related Work}
Applying Bayesian methods to neural networks has been studied in the past with various approximation methods for the intractable true posterior probability distribution $p(w|\mathcal{D})$. Buntine and Weigend \cite{buntine1991bayesian} started to propose various \textit{maximum-a-posteriori} (MAP) schemes for neural networks. They were also the first who suggested second order derivatives in the prior probability distribution $p(w)$ to encourage smoothness of the resulting approximate posterior probability distribution. In subsequent work by Hinton and Van Camp \cite{hinton1993keeping}, the first variational methods were proposed which naturally served as a regularizer in neural networks. Hochreiter and Schmidhuber \cite{hochreiter1995simplifying} suggest taking an information theory perspective into account and utilising a minimum description length (MDL) loss. This penalises non-robust weights by means of an approximate penalty based upon perturbations of the weights on the outputs. Denker and LeCun \cite{denker1991transforming}, and MacKay \cite{mackay1995probable} investigated the posterior probability distributions of neural networks by using Laplace approximations. As a response to the limitations of Laplace approximations, Neal \cite{neal2012bayesian} investigated the use of hybrid Monte Carlo for training neural networks, although it has so far been difficult to apply these to the large sizes of neural networks built in modern applications. More recently, Graves \cite{graves2011practical} derived a variational inference scheme for neural networks and Blundell et al. \cite{blundell2015weight} extended this with an update for the variance that is unbiased and simpler to compute. Graves \cite{graves2016stochastic} derives a similar algorithm in the case of a mixture posterior probability distribution. 
\newline Several authors have claimed that Dropout \cite{srivastava2014dropout} and Gaussian Dropout \cite{wang2013fast} can be viewed as approximate variational inference schemes \cite{gal2015bayesian, kingma2015variational}. We compare our results to Gal's \& Ghahramani's \cite{gal2015bayesian} and discuss the methodological differences in detail.


Several approaches have been proposed for Bayesian learning
of neural networks, based on, e.g., the Laplace approximation \cite{Mackay1991APB}, Hamiltonian Monte Carlo (Neal,
1995), expectation propagation (Jylanki et al., 2014) \cite{Jylanki:2014:EPN:2627435.2638593}, and Â¨
variational inference (Hinton & Camp, 1993) \cite{hinton1993keeping}. However,
these approaches have not seen widespread adoption due
to their lack of scalability in both network architecture
and data size. A notable exception is the scalable variational
inference approach of Graves (2011) \cite{graves2011practical}. However, this method seems to perform poorly in practice due to noise
from Monte Carlo approximations within the stochastic
gradient computations. A different scalable solution based
on expectation propagation was proposed by Soudry et al.
(2014) \cite{Soudry}. While this method works for networks with binary
weights, its extension to continuous weights is unsatisfying
as it does not produce estimates of posterior variance.



\begin{table}
\caption{Even better looking table using booktabs}
\centering
\label{table:good_table}
\begin{tabular}{l c c c c}
\toprule
\multirow{2}{*}{Dental measurement} & \multicolumn{2}{c}{Species I} & \multicolumn{2}{c}{Species II} \\ 
\cmidrule{2-5}
  & mean & SD  & mean & SD  \\ 
\midrule
I1MD & 6.23 & 0.91 & 5.2  & 0.7  \\

I1LL & 7.48 & 0.56 & 8.7  & 0.71 \\

I2MD & 3.99 & 0.63 & 4.22 & 0.54 \\

I2LL & 6.81 & 0.02 & 6.66 & 0.01 \\

CMD & 13.47 & 0.09 & 10.55 & 0.05 \\

CBL & 11.88 & 0.05 & 13.11 & 0.04\\ 
\bottomrule
\end{tabular}
\end{table}
